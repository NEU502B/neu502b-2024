{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU502B Homework 5\n",
    "*Due April 15, 2024*\n",
    "\n",
    "*Submission instructions:* First, rename your homework notebook to include your name (e.g. `homework-5-nastase.ipynb`); keep your homework notebook in the `homework` directory of your clone of the class repository. Prior to submitting, restart the kernel and run all cells (see *Kernel* > *Restart Kernel and Run All Cells...*) to make sure your code runs and the figures render properly. Only include cells with necessary code or answers; don't include extra cells used for troubleshooting. To submit, `git add`, `git commit`, and `git push` your homework to your fork of the class repository, then make a pull request on GitHub to sync your homework into the class repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first homework assignment, we explored how a system can extract latent structure in sensory stimuli (e.g. natural scenes) using unsupervised learning algorithms like Hebbian learning. Our model was shown a set of images with no final goal specified, nor any expectations with which to compare its performance throughout learning. Now, we're interested in how a system can learn to reach a goal through interactions with its environment, by maximizing rewards or minimizing penalties.\n",
    "\n",
    "Reinforcement learning (RL) models solve problems by maximizing some operationalization of reward. These models use goal-directed learning to solve closed-loop problems: present actions influence the environment, thus changing the circumstances of future actions toward the same goal. In RL, we hope to discover the actions that increase chances of rewards within specific states in the environment.\n",
    "\n",
    "<img src=\"model.png\" width=\"400\">  \n",
    "\n",
    "An agent must be able to sense the state of the environment either fully or partially, and its actions must be able to change this state. Consider the following example from Sutton and Barto (1992):\n",
    "> \"Phil prepares his breakfast. Closely examined, even this apparently mundane activity reveals a complex web of conditional behavior and interlocking goal-subgoal relationships: walking to the cupboard, opening it, selecting a cereal box, then reaching for, grasping, and retrieving the box. Other complex, tuned, interactive sequences of behavior are required to obtain a bowl, spoon, and milk jug. Each step involves a series of eye movements to obtain information and to guide reaching and locomotion. Rapid judgments are continually made about how to carry the objects or whether it is better to ferry some of them to the dining table before obtaining others. Each step is guided by goals, such as grasping a spoon or getting to the refrigerator, and is in service of other goals, such as having the spoon to eat with once the cereal is prepared and ultimately obtaining nourishment.\"\n",
    "\n",
    "At each point in time, there is a state-action pair. Some of them fall under sub-goals, while others could ultimately be a state where there is a high chance of reward, fulfilling the goal of feeding. To be able to model this process, we have to break it down into its interacting components: \n",
    "- The agent has a policy, the map between perceived states and the actions taken. We can think of it as a set of stimulus-response rules or associations that determine behavior given a state and a goal within the environment. It can be implemented through the probabilities of taking specific actions given a state. \n",
    "- This set of rules should serve to maximize the reward signal in the short and/or long term. \n",
    "- Environmental states are evaluated through a value function, which provides a measure of the expected rewards that can be obtained moving forward from a specific state. Grabbing a bowl might not feed you immediately, yet it has high value as it will lead you to a state in which you can feed yourself some cereal without spilling milk all over the table. Would grabbing a shallow dish instead of a bowl have the same value? Actions are taken based on these value judgements. \n",
    "- The agent could have the ability for foresight and planning if it has a model of the environment. This means it can have a model of how the environment reacts to its behavior, from which to base its strategies and adjustments.\n",
    "\n",
    "At each decision, the agent has a choice to either exploit the actions it has already tested to be effective, or it can explore the action-state space to find new routes to optimal rewards. Exploration is risky, yet under some circumstances it will pay off in the long run. Finding the balance between the two would be the optimal solution in uncertain environments. Different methods can be employed to deal with this duality:\n",
    "- On-policy methods improve the policy that is used to make decisions. This policy is generally soft (probabilistic), as $P(s∈S,a∈A│s)>0$, where $S$ is the possible states and $A|s$ is the possible actions given a state. The probability is gradually shifted to a deterministic optimal policy with each update. For example, $\\epsilon-greedy$ policies choose an action that has maximal expected value most of the time (with probability 1 – a small number $\\epsilon$). However, with probability $\\epsilon$ the agent will choose an action at random. The agent will try to learn values based on subsequent optimal behavior, yet it has to behave non-optimally (choosing random actions) in order to explore and find the optimal actions. This means the agent has to learn about the optimal policy while behaving according to an exploratory policy. On-policy can be thought of as a compromise, where values are learned for a near optimal policy that still explores. \n",
    "- Another approach is to use two policies, a target policy and a behavior policy. The first one is modified based on the information that is collected through the behaviors generated by the second. This approach is termed off-policy, as learning occurs based on behaviors generated off the policy being developed. The benefit here is that the target policy can be deterministic (i.e. greedy), while the behavior policy can continue to explore without limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Fitting RL models to data\n",
    "\n",
    "First, familiarize yourself with the two-step RL task ([Daw et al., 2011](https://doi.org/10.1016/j.neuron.2011.02.027)). Visit this website to play through an example of the two-step RL task: https://nivlab.github.io/jspsych-demos/tasks/two-step/experiment.html. If you're interested, the Python code for the task can be found at: https://github.com/nivlab/jspsych-demos/tree/main/tasks/two-step.\n",
    "    \n",
    "The data from the two-step task are structured as follows:\n",
    "- **choice1**: your choices at the first level (1 or 2)\n",
    "- **choice2**: your choices at the second level (1 or 2)\n",
    "- **state**: which second level game you were offered on this trial \n",
    "    - choice1 = 1 at the first level (S1) leads to S2 in approximately 70% of the trials\n",
    "    - choice1 = 2 at the first level (S1) leads to S3 in approximately 70% of the trials\n",
    "- **money**: did you get a reward on each trial or not (0 or 1)\n",
    "\n",
    "Note that missed trials will have a 0 in the choice; trials can be missed either at the first or second level. When you write your code (later on), make sure to deal separately with missed trials as this is a common source of discrepancies while fitting the models. Below is a schematic representation of the task structure:\n",
    "\n",
    "\n",
    "<img src=\"task.png\" width=\"400\">\n",
    "\n",
    "|       | $A_1$     | $A_2$     |\n",
    "|:-----:|:---------:|:---------:|\n",
    "| $S_1$ | $Q_{S1,A1}$ | $Q_{S1,A2}$ |\n",
    "| $S_2$ | $Q_{S2,A1}$ | $Q_{S2,A2}$ |\n",
    "| $S_3$ | $Q_{S3,A1}$ | $Q_{S3,A2}$ |\n",
    "\n",
    "\n",
    "The schematic does not map to the colors used in the actual task.  S1 refers to the state at the top (first) level, where you will be shown two distinct rocket ships. You will have to choose one of the two (represented by action | state in the schematic). One of the rockets, let’s say A1|S1 will have 70% chance of transferring you to S2 (one of the possible states at the bottom level), and a 30% chance of getting you to S3. This is represented by the thickness of the arrows. For A2|S1, the chances are inverted. At the bottom (second) level, you can be at either of two distinct states (S2 or S3). You will need to choose between two aliens at each state, with gradually drifting chances of getting a reward once a decision is made. For example, A1|S2 might start with higher chances than A2|S2. These probabilities will change gradually with time and at some point, the chances might be reversed. There is no implicit relationship between what happens in S2 and S3. You will have to learn, with each experience, which choices lead you to better rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the data for one of our subjects. Make sure you understand what each variable contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('sub-0.npz')\n",
    "\n",
    "c1 = data['choice1']\n",
    "c2 = data['choice2']\n",
    "s = data['state']\n",
    "m = data['money']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the bottom level, learning can be modeled with $Q$-learning or Rescorla-Wagner learning, as there's no future state. Note that these learning rules are identical if you treat each option as an action (in $Q$-learning) or as a state (the state of the chosen stimulus, in Rescorla-Wagner).\n",
    "\n",
    "> **$Q$-learning**: $Q^{new}(a|s) \\leftarrow Q(a|s) + \\eta * (R_t-Q(a|s))$ \n",
    "\n",
    "> **Rescorla-Wagner learning**: $V_{t+1} \\leftarrow V_T + \\eta * (R_T-V_T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Describe in words the variables in each equation how each the learning rule \"works\":\n",
    "\n",
    "> *Q-learning*\n",
    "> \n",
    "> Qnew(a|s) is the new, or updated value of a action a given state s. In other words, there is a model of the value the agent expects to receive when taking a given action in a given state.\n",
    ">\n",
    "> Q(a|s) is the expected value of acion a given state s prior to the value representation being updated by the reward received from the environment. This is the value expectation used to make the action decision.\n",
    ">\n",
    "> n is the learnign rate.\n",
    ">\n",
    "> Rt is the reward received at timestep t.\n",
    ">\n",
    "> *rescorla-Wagner learning*\n",
    "> \n",
    "> Vt+1 is the value prediction for the next timestep (timestep is defined vaguely here depending on how the task is paramaterized, could be literal timestep, could be trial).\n",
    ">\n",
    "> VT is the current value prediction for the current time T.\n",
    ">\n",
    "> n is the learning rate.\n",
    ">\n",
    "> RT is the reward received by the environment.\n",
    ">\n",
    "> As paramaterized here, given the context of this task, both Q learnign and RW learnign function in much the same way. You start with some expectation of reward, and use it to pick an action. That action returns a reward from the environment whose difference to the expectation is scaled by a learnign rate and used to update the expectation. The difference is that Q-learning is more flexibile in that it explicitly maps expected value to an action given a state (or observation). This mapping can be learned via a neural network, as in the differentiable neural computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the top level, learning can be modeled in several different ways. We'll consider two: (1) **model-free learning** and (2) **model-based learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **model-free learning**, we'll start with the temporal difference (TD) learning rule.\n",
    "\n",
    "> **TD(0)**: $V_T \\leftarrow V_T + \\eta*(R_T+\\gamma*V_{T+1}-V_T)$\n",
    "\n",
    "Here, $R_T=0$ because the first state doesn't yield rewards and $\\gamma$ is the temporal discount parameter of future rewards—this allows us to adjust the first-stage actions by taking into account the result of the second-stage action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to make learning more efficient is to use TD($\\lambda$) instead of TD(0) learning. In this case, we add an additional memory variable associated with each state to serve as an \"eligibility trace\". You can think of it as a \"memory\" that a particular state has been visited, which decays (e.g. exponentially) over time. Every time a state is visited, its eligibility trace becomes 1; at every subsequent time point, the eligibility trace is multiplied by a factor $0 < \\lambda \\leq 1$. At the end of a trial or episode, all eligibility traces become 0.\n",
    "\n",
    "All states are updated according to *learning rate $\\cdot$ prediction error $\\cdot$ eligibility trace*. This will automatically update all the states visited in this episode (i.e. all the states \"eligible\" for updating), doing so for the most recently visited states to a greater extent. Write the updated equation.\n",
    "\n",
    "> **TD($\\lambda$)**: $V_{T+1} = V_T + \\eta * E(\\lambda)*(R_T+\\gamma V_{T+1}-V_T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Again, describe the variables in these equations and how the learning rules \"work\":\n",
    "\n",
    "> *Memory temporal difference learning*\n",
    "> \n",
    "> Vt+1 is the expected value at time point t+1 (again, definning time point flexibly here).\n",
    ">\n",
    "> Vt is the expected value at the current time point.\n",
    ">\n",
    "> n is the learning rate.\n",
    ">\n",
    "> E(lambda) is the eligibility trace for the last time the current state was occupied. This function, E(.), decays the trace of being in this state by some function (decays from 1 to 0) over time. The point is that this memory term modulates the update associated with being in this state.\n",
    ">\n",
    "> Rt is the reward received at time point t.\n",
    ">\n",
    "> Gamma is a temporal discount parameter (rewards experienced now are worth more than rewards experienced far--temporally speaking--from now). This term reflects the fact that temporally remote rewards are worth less than temporally proximal rewards in the sense that remote rewards are discounted more form their full value.\n",
    ">\n",
    "> Vt+1 is expected reward at next time step.\n",
    ">\n",
    "> In sum, this modified TD learning rule adds terms to account for temporal discounting of future expected rewards, which helps guide behavior to longer-term goals), and a memory eligibility trace that is useful for modulating the learnign update based on how much experience has been had with the current state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **model-based learning**, let's begin by assuming that transition model (i.e. the probabilities of going from $S1$ to $S2$ or $S3$ given choice1) is known from the start—while the reward model is not known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How can you use the transition probabilities and the learned values at the second-stage states to plan and make choices at the first stage? How would you implement this model?\n",
    "\n",
    "> To reiterate, I'm in state 1 and make choice 1. Choice 1 has probability x of resulting in state 2, and 1-x of resulting in state 3. I have learned some representation of the rewards received by being in states 2 and 3. Figuring out what to do can therefore be construed as a matter of multiplying the rewards received at each of states 2 and 3 (or at least expected to be received) by the probabilities of winding up in those states given the choices I have. This model would have me make the choice that has the highest probability of windning up in the state with the highest reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How many parameters do each the four models have?\n",
    "\n",
    "> All four models have the learning rate parameter. The two TD learning algorithms, though, add a discount factor, bring their number of parameters up to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll implement and fit the following models. Implement TD($\\lambda$) using the $Q$-learning and State-Action-Reward-State-Action (SARSA) algorithms. Some pseudocode is provided to get you started. These algorithms use state-action value predictions ($Q$ values) to choose actions. In state $S$, the algorithm chooses an action according to softmax $Q$ values.\n",
    "\n",
    "<img src=\"Pas.png\" width=\"550\"> \n",
    "\n",
    "Here, $β$ is an inverse-temperature parameter that we'll optimize. If you're using constrained optimization, fix $β$ to be in the range [0, 100].\n",
    "\n",
    "Update the eligibility traces. Recall that the eligibility traces are values corresponding to each state and action pair, and are set to zero at the beginning of the trial. Upon taking action $a$ to leave state $S$ for state $S^{new}$ and receiving reward $r$, the eligibility traces $e(a│S)$ are updated for each $(S, a)$ pair:\n",
    "\n",
    "<img src=\"eaS.png\" width=\"300\">\n",
    "\n",
    "All $Q(a|S)$ are updated according to:\n",
    "\n",
    "<img src=\"Q.png\" width=\"250\">\n",
    "\n",
    "With prediction error $δ(t)$ being: \n",
    "\n",
    "<img src=\"delta.png\" width=\"400\">\n",
    "\n",
    "The parameter $η$ is a step-size or learning-rate parameter in the range (0,1]. \n",
    "\n",
    "Reset the eligibility traces to 0 at the end of each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_nll(params, state, choice1, choice2, money, model_based=False):\n",
    "    \"\"\"\n",
    "    Compute nll for different RL strategies (either SARSA or Q learning.\n",
    "    \"\"\"\n",
    "    eta, beta, lambd = params\n",
    "    n_states = 3 # For s1, s2, s2.\n",
    "    n_actions = 2 # At any state, you have two possible actions.\n",
    "    n_trials = s.shape[0]\n",
    "    \n",
    "    # Initialize an array to store Q-values that is size n_states x n_actions\n",
    "    Q = np.ones((n_states, n_actions))\n",
    "    \n",
    "    # Initialize log-likelihood\n",
    "    LL = 0 # ¯\\_(ツ)_/¯.\n",
    "    \n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        # Create an n_states x n_actions matrix to store your eligibility traces for the current trial\n",
    "        E = np.zeros((n_states, n_actions))\n",
    "        \n",
    "        # Get your current state for the top level (S1)\n",
    "        S = 0\n",
    "        \n",
    "        # Stop if trial was missed. Missed trials will have a value of -1.\n",
    "        if choice1[t] == -1:\n",
    "            continue # Freak out.\n",
    "                \n",
    "        # First level choice likelihood: compute likelihood of choice at the first state S1. \n",
    "        # Your likelihood should be a softmax function.\n",
    "        Q_curr = beta * Q[S, choice1[t]]\n",
    "        Q_all = beta * Q[S, :]\n",
    "        p_chosen = np.exp(Q_curr) / np.sum(np.exp(Q_all))\n",
    "        # Update the log likelihood\n",
    "        LL += np.log(p_chosen)\n",
    "\n",
    "        # Learning at first level: update your eligibility trace according to\n",
    "        # e(a|S) = 1                       for the chosen action (a) in the current state (S)\n",
    "        # e(a|S) = lambda * e(a|S)         for all other a, S pairs\n",
    "        E = lambd * E\n",
    "        E[S, choice1[t]] = 1\n",
    "        \n",
    "        # Update prediction error without reward (because we are in the first level)\n",
    "        if model_based:\n",
    "            # Implement SARSA update for model based learning\n",
    "            # Keep in mind that choosing 1 at the first level (S1) leads to S2 in approximately 70% of the trials \n",
    "            # and choosing 2 at the first level (S1) leads to S3 in approximately 70% of the trials\n",
    "            Q2 = Q[1, :]\n",
    "            Q3 = Q[2, :]\n",
    "\n",
    "            # Set probabilities of outcomes.\n",
    "            if choice1[t] == 0:\n",
    "                p2 = 0.7\n",
    "                p3 = 0.3\n",
    "            else:\n",
    "                p2 = 0.3\n",
    "                p3 = 0.7\n",
    "\n",
    "            PE = p2 * Q2.max() + p3 * Q3.max() - Q[S, choice1[t]]\n",
    "\n",
    "        else:\n",
    "            # Implement Q-learning update for model free learning\n",
    "            PE = Q[state[t]].max() - Q[S, choice1[t]]\n",
    "        \n",
    "        # update Q values according to Q = Q + eta * prediction errror * eligibility\n",
    "        Q = Q + eta * PE * E\n",
    "        \n",
    "        # Get your current state for the second level (S2 or S3)\n",
    "        S = state[t] \n",
    "        \n",
    "        # Stop if trial was missed at the second level. Missed trials will have a value of -1\n",
    "        if choice2[t] == -1:\n",
    "            continue\n",
    "            \n",
    "        # Second level choice likelihood: compute likelihood of choice at the second state (S2 or S3). \n",
    "        # Your likelihood should be a softmax function.\n",
    "        Q_curr = beta * Q[state[t], choice2[t]]\n",
    "        Q_all = beta * Q[state[t], :]\n",
    "        p_chosen = np.exp(Q_curr) / np.sum(np.exp(Q_all))\n",
    "        # Update the log likelihood\n",
    "        LL += np.log(p_chosen)\n",
    "\n",
    "        # Learning at second level: update your eligibility trace according to\n",
    "        # e(a|S) = 1                       for the chosen action (a) in the current state (S)\n",
    "        # e(a|S) = lambda * e(a|S)         for all other a, S pairs\n",
    "        E = lambd * E\n",
    "        E[state[t], choice2[t]] = 1\n",
    "        \n",
    "        # Update the prediction error with reward because we are in the second level\n",
    "        # NOTE: This update IS NOT dependent on the next state because we are in the final state\n",
    "        PE = money[t] - Q[state[t], choice2[t]]\n",
    "        \n",
    "        # update Q values according to Q = Q + eta * prediction errror * eligibility\n",
    "        Q = Q + eta * PE * E\n",
    "    \n",
    "    return -LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is the prediction error ($\\delta$) update in the second stage fundamentally similar or different between $Q$-learning and SARSA? Explain your answer.\n",
    "\n",
    "> They are the same because the second stage is the final state so no consideration of future predictions are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which of these two algorithms is considered on-policy, which is off-policy, and why?\n",
    "\n",
    "> SARSA is on policy and Q learning is off. This is because learning, for SARSA, has the effect of updating the model itself based on expected value predictions made by the model itself (the whole Vt+1 thing). In effect, SARSA uses the same model for updating and predicting at each time step. Q learning does not--its predictions are agnostic to the model being used to select the current action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each subject, load in their data as described at the beginning of the assignment (`sub-0.npz` to `sub-4.npz`). The `sub-0.npz` file contains sample data, while the rest are experimental data collected from other students at PNI. Use SciPy's [`minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) function (imported at the beginning of the problem set) to fit the two models to each of the subjects. You may also want to keep of the number of trials completed by each subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "np.random.seed(1312)\n",
    "params = [.5, 50, .5]\n",
    "bounds = [(0, 1), (0, 100), (0, 1)]\n",
    "sub_fns = ['sub-0.npz', 'sub-1.npz', 'sub-2.npz',\n",
    "           'sub-3.npz', 'sub-4.npz']\n",
    "\n",
    "# Example solver method for SciPy's minimize\n",
    "method = 'TNC'\n",
    "\n",
    "# Loop through subjects, load data, and fit models:\n",
    "SARSA_sub_nll = []\n",
    "Qlearning_sub_nll = []\n",
    "sub_num_trials = []\n",
    "for sub in sub_fns:\n",
    "\n",
    "    # Load data and variables.\n",
    "    data = np.load(sub)\n",
    "    c1 = data['choice1']\n",
    "    c2 = data['choice2']\n",
    "    s = data['state']\n",
    "    m = data['money']\n",
    "\n",
    "    # SARSA.\n",
    "    neg_ll = minimize(rl_nll, params, args=(s, c1, c2, m, True), method=method, bounds=bounds).fun\n",
    "    SARSA_sub_nll.append(neg_ll)\n",
    "\n",
    "    # Q.\n",
    "    neg_ll = minimize(rl_nll, params, args=(s, c1, c2, m, False), method=method, bounds=bounds).fun\n",
    "    Qlearning_sub_nll.append(neg_ll)\n",
    "\n",
    "    sub_num_trials.append(len(c1[(c1 != -1) & (c2 != -1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Bayesian information criterion (BIC) to compare which is the best-fitting model for each subject. Compute BICs using the following formula:\n",
    "\n",
    "> $BIC = -2 * \\text{log-likelihood} + \\ln(\\text{number of trials}) *  \\text{number of parameters}$\n",
    "\n",
    "where $\\ln()$ is the natural logarithm. BIC is defined here on the deviance scale, which means that lower values are better. **Question**: Which model fits each subject's behavior best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BIC for each model and subject:\n",
    "get_BICed = lambda nll, n_trials: -2 * -1 * nll + np.log(n_trials) * 3\n",
    "\n",
    "SARSA_BICs = []\n",
    "Qlearning_BICs = []\n",
    "for i in range(len(sub_fns)):\n",
    "    SARSA_BICs.append(get_BICed(SARSA_sub_nll[i], sub_num_trials[i]))\n",
    "    Qlearning_BICs.append(get_BICed(Qlearning_sub_nll[i], sub_num_trials[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0414ffe960>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(SARSA_BICs, label=\"SARSA\")\n",
    "plt.plot(Qlearning_BICs, label=\"Q\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SARSA is better for subjects 4 and 5. Q learning is better for 1, 2, and 3.\n",
    "> But Declan said the data is bad soooooooo......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Cliff walking\n",
    "\n",
    "Consider the grid world shown below. This is a standard undiscounted, episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. Reward is -1 on all transitions except those into the region marked “The Cliff.” Stepping into this region incurs a reward of -100 and sends the agent instantly back to the start.\n",
    "\n",
    "<img src=\"cliff.png\" width=\"550\">  \n",
    "\n",
    "Two paths are marked: an optimal path which incurs the least costs on the way to the goal, and a roundabout (but safe) path that walks farthest from the cliff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which algorithm, SARSA or $Q$-learning, would learn either path, and why?\n",
    "\n",
    "> Q learning will converge to the optimal path and SARSA to the safe path. This is because Q learning factors-in the subsequent action with the max reward, which will lead it to the optimal path. SARSA, by contrast, has a stochastic term that will lead it to take a random action a certain percent of the time. The optimal route therefore maximizes the probability the SARSA agent will fall off the cliff, leading it to learn to take the safe path which miniizes the probability stochastic actions will lead to falling off the cliff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: When behaving according to the softmax of the learned $Q$ values, which path would an agent prefer? (Consider the parameter $β$ and the stability of the environment.) \n",
    "\n",
    "> The optimal path. Especially since the denominator effect of softmax is likely to accentuate the bias towards the most rewarding option as compared to the alternatives in converting the various options to a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Can you explain why on-policy methods might be superior for learning real-world motor behavior?\n",
    "\n",
    "> Because Q learning assumes completely rational (in an economic sense) behavior at all times, which is not ecological. There is stochasticity in animal behavior, which is built into SARSA. With that said, you may quibble about the formulation of that stochasticity. Nonetheless, the output of that randomness can take the form of minimizing risk/maintaining flexibility for exploration. Both are features of ecological behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P., & Dolan, R. J. (2011). Model-based influences on humans' choices and striatal prediction errors. *Neuron*, *69*(6), 1204–1215. https://doi.org/10.1016/j.neuron.2011.02.027\n",
    "\n",
    "* Sutton, R. S., & Barto, A. G. (1992). Reinforcement Learning: An Introduction. MIT Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
